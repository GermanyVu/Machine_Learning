{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# selecting the available device (cpu/gpu)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0144090790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1',render_mode='rgb_array')\n",
    "seed = 100\n",
    "env.reset(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "number_of_episodes = 700\n",
    "max_steps = 1000\n",
    "learning_rate = .01\n",
    "discount_factor = .99\n",
    "hidden_layer_size = 64\n",
    "egreedy = .9\n",
    "egreedy_final = .02\n",
    "egreedy_decay = 500\n",
    "replay_buffer_size = 2000\n",
    "batch_size = 32\n",
    "update_target_frequency = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of States : 4\n",
      "Total number of Actions : 2\n"
     ]
    }
   ],
   "source": [
    "number_of_states = env.observation_space.shape[0]\n",
    "\n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "print('Total number of States : {}'.format(number_of_states))\n",
    "\n",
    "print('Total number of Actions : {}'.format(number_of_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon decay\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    '''\n",
    "    function for decaying the epsilon values after every step\n",
    "    \n",
    "    decays epsilon with increasing steps\n",
    "    \n",
    "    param: stepsdone(int): number of steps completed\n",
    "    \n",
    "    returns: int - decayed epsilon\n",
    "    \n",
    "    '''\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final)\\\n",
    "                *np.exp(-1.0* steps_done/egreedy_decay)\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASStorch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)[SOURCE]\n",
    "Applies a linear transformation to the incoming data: y = xA^T + by=xA \n",
    "T\n",
    " +b\n",
    "\n",
    "This module supports TensorFloat32.\n",
    "\n",
    "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.\n",
    "\n",
    "Parameters:\n",
    "in_features (int) – size of each input sample\n",
    "\n",
    "out_features (int) – size of each output sample\n",
    "\n",
    "bias (bool) – If set to False, the layer will not learn an additive bias. Default: True\n",
    "\n",
    "\n",
    "m = nn.Linear(20, 30)\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input)\n",
    "print(output.size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " accepts the number of states as inputs and outputs Q values for the \n",
    " number of actions present in the environment\n",
    " \n",
    " as a network with a hidden layer of size \n",
    "'''\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, hidden_layer_size):\n",
    "        super().__init__() # init of the super class \n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.fc1 = nn.Linear(number_of_states,self.hidden_layer_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_layer_size,number_of_actions)\n",
    "   \n",
    "    def forward(self,x):\n",
    "        output = torch.tanh(self.fc1(x))\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    " ''' \n",
    "the constructor will contain the following variables:\n",
    "    -capacity, which indicates the maximum size of the replay buffer\n",
    "    -buffer, which is an empty Python list that acts as the memory buffer\n",
    "    - pointer, which points to the current location of the memory buffer while\n",
    "      pushing the memory to the buffer\n",
    "\n",
    "The class will contain the push function, which checks whether there is any \n",
    "space in the buffer using the pointer variable\n",
    "    -If there is an empty space, push adds an experience tuple at the end of \n",
    "     the buffer,\n",
    "    -else the function will replace the memory from the \n",
    "     starting point of the buffer\n",
    "        \n",
    "sample function, which will return the experience tuple of the batch size, and \n",
    "\n",
    "the __len__ function, which will return the length of the current buffer,\n",
    "'''\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.pointer = 0\n",
    "        \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        experience = (state, action, new_state, reward, done)\n",
    "        if self.pointer >= len(self.buffer):\n",
    "            self.buffer.append(experience)\n",
    "        else: \n",
    "            self.buffer[self.pointer] = experience\n",
    "            self.pointer = (self.pointer+1)%self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.buffer, batch_size))\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "            \n",
    "               \n",
    "    \n",
    "'''\n",
    "function will create an instance of the DQN class within which the hidden \n",
    "layer size is passed. \n",
    "\n",
    "define the MSE as a loss criterion\n",
    " \n",
    "define Adam as the optimizer with model parameters and a predefined\n",
    " learning rate\n",
    " \n",
    "Create a replica of the normal DQN network and name it target_dqn.\n",
    "    \n",
    "target_dqn_update_counter to periodically update the weights of the \n",
    "target DQN from the DQN network. \n",
    "\n",
    "Add the following steps. memory.sample(BATCH_SIZE) will randomly\n",
    "pull the experiences from the replay buffer for training\n",
    "\n",
    "Pass new_state in the target network to get the target Q values \n",
    "from the target network.\n",
    "\n",
    "Finally, update the weights of the target network from the normal or predicted DQN after a \n",
    "certain iteration is specified in UPDATE_TARGET_FREQUENCY\n",
    "'''   \n",
    "    \n",
    "class DQN_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.dqn = DQN(hidden_layer_size).to(device) #moves model to cpu or gpu\n",
    "        self.target_dqn = DQN(hidden_layer_size).to(device)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.dqn.parameters(), \\\n",
    "                                                lr=learning_rate)\n",
    "        self.target_dqn_update_counter = 0 \n",
    "        \n",
    "\n",
    "    '''\n",
    "    The select_action function passes the state through \n",
    "    the DQN to obtain the Q values\n",
    "\n",
    "    selects the action with the highest\n",
    "    Q value during exploitation\n",
    "\n",
    "    The if statement decides whether the exploration \n",
    "    should be carried out or not\n",
    "\n",
    "    function will pass the state through the DQN to get the Q value\n",
    "\n",
    "    select the action with the highest Q value using the torch.max operation \n",
    "    during the exploitation phase\n",
    "\n",
    "    During this process, gradient computation is not required; that's why we use \n",
    "    the torch.no_grad() function to turn off the gradient calculation\n",
    "\n",
    "    '''\n",
    "\n",
    "    def select_action(self,state, egreedy):\n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        if random_for_egreedy > egreedy: #will become more likely with more steps\n",
    "            with torch.no_grad():\n",
    "                state = torch.Tensor(state).to(device)#moves tensor to cpu or gpu\n",
    "                q_values = self.dqn(state)\n",
    "                action = torch.max(q_values,0)[1]# will get only indices of highest value\n",
    "                action = action.item() # gets float from object tensor\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    If the episode is ended, the target Q value will be the reward\n",
    "    obtained; \n",
    "\n",
    "    otherwise, use the Bellman equation to estimate the target Q value\n",
    "\n",
    "\n",
    "    if the episode ended then the loss will be [Q(s,a;theta) - r]^2\n",
    "\n",
    "    otherwise\n",
    "    [Q(s,a,theta) - (r(s.a)+gamma*maxQ(s',a,theta))]^2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    def optimize(self):\n",
    "        \n",
    "        if (batch_size > len(memory)):\n",
    "            return \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = torch.Tensor(state).to(device) #moves tensor to cpu or gpu\n",
    "       \n",
    "        new_state = torch.Tensor(new_state).to(device)\n",
    "        reward = torch.Tensor([reward]).to(device)\n",
    "        if done: #finished game no more steps needed\n",
    "            target_value = reward\n",
    "        else:\n",
    "            new_state_values = self.dqn(new_state).detach() #tensor.detach() creates a tensor that shares storage\n",
    "                                                            #with tensor that does not require grad. It detaches \n",
    "                                                            #the output from the computational graph. \n",
    "                                                            #So no gradient will be backpropagated along this variabl\n",
    "\n",
    "            max_new_state_values = torch.max(new_state_values)\n",
    "            target_value = reward + discount_factor * max_new_state_values\n",
    "\n",
    "        predicted_value = self.dqn(state)[action].view(-1)\n",
    "        loss = self.criterion(predicted_value, target_value)\n",
    "\n",
    "        self.optimizer.zero_grad() #  sets all gradients to zero\n",
    "        loss.backward() #loss.backward() computes dloss/dx for every parameter \n",
    "                        #x which has requires_grad=True.  x.grad += dloss/dx\n",
    "        self.optimizer.step() #optimizer.step updates the value of x using the gradient x.grad. \n",
    "                            #x += -lr * x.grad gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQN_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "instantiate the DQN agent using the class created earlier. \n",
    "\n",
    "Create a steps_total empty list to collect the total number of \n",
    "steps for each episode.\n",
    "\n",
    "Initialize steps_counter with zero and use it to calculate the\n",
    "decayed epsilon value for each step.\n",
    "\n",
    "Use two loops during the training process\n",
    "    -The first one is to play the game for a certain number of steps.\n",
    "    \n",
    "    -The second loop ensures that each episode goes on for a fixed number of steps.\n",
    "    \n",
    "Using the present state and epsilon value, you select the action to perform. \n",
    "\n",
    "Once you take the action, the environment returns the new_state, reward, and done flags\n",
    "\n",
    "Using the optimize function, perform one step of gradient descent to optimize the DQN. \n",
    "\n",
    "Now make the new state the present state for the next iteration.\n",
    "\n",
    "check whether the episode is over or not. If the episode is over, you can \n",
    "collect and record the reward for the current episode\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# instantiating the DQN Agent\n",
    "\n",
    "dqn_agent = DQN_Agent()\n",
    "steps_total = []\n",
    "steps_counter = 0 \n",
    "for episode in range(number_of_episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    step = 0\n",
    "    for idx in range(max_steps):\n",
    "        step += 1\n",
    "        steps_counter += 1\n",
    "        egreedy = calculate_epsilon(steps_counter) # decays until all episodes are fin\n",
    "        action = dqn_agent.select_action(state, egreedy)\n",
    "        new_state, reward, done, _, info = env.step(action)\n",
    "        dqn_agent.optimize(state, action, new_state, reward, done)\n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            break\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 150.20\n",
      "Average reward (last 100 episodes): 375.43\n"
     ]
    }
   ],
   "source": [
    "print(\"Average reward: %.2f\" % (sum(steps_total)/number_of_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Rewards Collected\")\n",
    "plt.bar(np.arange(len(steps_total)), steps_total, alpha=0.5, color='green', width=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
